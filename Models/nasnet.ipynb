{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-24T18:04:23.548684808Z",
     "start_time": "2024-03-24T18:04:20.119190749Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 14:04:20.470352: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-24 14:04:20.736652: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-24 14:04:20.736676: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-24 14:04:20.737851: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-24 14:04:20.847465: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-24 14:04:21.760869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4070 Laptop GPU, compute capability 8.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 14:04:23.434341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 14:04:23.541050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 14:04:23.541332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 14:04:23.541771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import utils\n",
    "from tensorflow.keras import mixed_precision\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data_path = \"BIRDS1_split/train\"\n",
    "test_data_path = \"BIRDS1_split/test\"\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
    "BATCH_SIZE = 32\n",
    "bird_classes = sorted(os.listdir(train_data_path))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:04:27.685228714Z",
     "start_time": "2024-03-24T23:04:27.671134735Z"
    }
   },
   "id": "6ef16eed851f94c6",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25744 files belonging to 167 classes.\n",
      "Using 18021 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 19:04:29.692564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 19:04:29.692915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 19:04:29.693073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 19:04:29.759244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 19:04:29.759432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 19:04:29.759592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 19:04:29.759726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6069 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-03-24 19:04:30.025846: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25744 files belonging to 167 classes.\n",
      "Using 7723 files for validation.\n",
      "Found 11545 files belonging to 167 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_data_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.3,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_data_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    shuffle=False,\n",
    "    seed=123,\n",
    "    validation_split=0.3,\n",
    "    subset='validation'\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_data_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    shuffle=False,\n",
    "    seed=123,\n",
    "\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:04:31.586973739Z",
     "start_time": "2024-03-24T23:04:28.913929483Z"
    }
   },
   "id": "727dfbfe4c00fe25",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.nasnet.preprocess_input\n",
    "train_dataset = train_dataset.map(lambda x, y: (preprocess_input(x), y))\n",
    "validation_dataset = validation_dataset.map(lambda x, y: (preprocess_input(x), y))\n",
    "test_dataset = test_dataset.map(lambda x, y: (preprocess_input(x), y))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:04:31.618393484Z",
     "start_time": "2024-03-24T23:04:31.591344490Z"
    }
   },
   "id": "10dd6962d0bf3fe8",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_3\n",
      "stem_conv1\n",
      "stem_bn1\n",
      "activation_188\n",
      "reduction_conv_1_stem_1\n",
      "reduction_bn_1_stem_1\n",
      "activation_189\n",
      "activation_191\n",
      "separable_conv_1_pad_reduction_left1_stem_1\n",
      "separable_conv_1_pad_reduction_right1_stem_1\n",
      "separable_conv_1_reduction_left1_stem_1\n",
      "separable_conv_1_reduction_right1_stem_1\n",
      "separable_conv_1_bn_reduction_left1_stem_1\n",
      "separable_conv_1_bn_reduction_right1_stem_1\n",
      "activation_190\n",
      "activation_192\n",
      "separable_conv_2_reduction_left1_stem_1\n",
      "separable_conv_2_reduction_right1_stem_1\n",
      "activation_193\n",
      "separable_conv_2_bn_reduction_left1_stem_1\n",
      "separable_conv_2_bn_reduction_right1_stem_1\n",
      "separable_conv_1_pad_reduction_right2_stem_1\n",
      "activation_195\n",
      "reduction_add_1_stem_1\n",
      "separable_conv_1_reduction_right2_stem_1\n",
      "separable_conv_1_pad_reduction_right3_stem_1\n",
      "activation_197\n",
      "separable_conv_1_bn_reduction_right2_stem_1\n",
      "separable_conv_1_reduction_right3_stem_1\n",
      "separable_conv_1_reduction_left4_stem_1\n",
      "activation_194\n",
      "separable_conv_1_bn_reduction_right3_stem_1\n",
      "separable_conv_1_bn_reduction_left4_stem_1\n",
      "reduction_pad_1_stem_1\n",
      "separable_conv_2_reduction_right2_stem_1\n",
      "activation_196\n",
      "activation_198\n",
      "reduction_left2_stem_1\n",
      "separable_conv_2_bn_reduction_right2_stem_1\n",
      "separable_conv_2_reduction_right3_stem_1\n",
      "separable_conv_2_reduction_left4_stem_1\n",
      "adjust_relu_1_stem_2\n",
      "reduction_add_2_stem_1\n",
      "reduction_left3_stem_1\n",
      "separable_conv_2_bn_reduction_right3_stem_1\n",
      "reduction_left4_stem_1\n",
      "separable_conv_2_bn_reduction_left4_stem_1\n",
      "reduction_right5_stem_1\n",
      "zero_padding2d_4\n",
      "reduction_add3_stem_1\n",
      "add_4\n",
      "reduction_add4_stem_1\n",
      "cropping2d_4\n",
      "reduction_concat_stem_1\n",
      "adjust_avg_pool_1_stem_2\n",
      "adjust_avg_pool_2_stem_2\n",
      "activation_199\n",
      "adjust_conv_1_stem_2\n",
      "adjust_conv_2_stem_2\n",
      "reduction_conv_1_stem_2\n",
      "concatenate_4\n",
      "reduction_bn_1_stem_2\n",
      "adjust_bn_stem_2\n",
      "activation_200\n",
      "activation_202\n",
      "separable_conv_1_pad_reduction_left1_stem_2\n",
      "separable_conv_1_pad_reduction_right1_stem_2\n",
      "separable_conv_1_reduction_left1_stem_2\n",
      "separable_conv_1_reduction_right1_stem_2\n",
      "separable_conv_1_bn_reduction_left1_stem_2\n",
      "separable_conv_1_bn_reduction_right1_stem_2\n",
      "activation_201\n",
      "activation_203\n",
      "separable_conv_2_reduction_left1_stem_2\n",
      "separable_conv_2_reduction_right1_stem_2\n",
      "activation_204\n",
      "separable_conv_2_bn_reduction_left1_stem_2\n",
      "separable_conv_2_bn_reduction_right1_stem_2\n",
      "separable_conv_1_pad_reduction_right2_stem_2\n",
      "activation_206\n",
      "reduction_add_1_stem_2\n",
      "separable_conv_1_reduction_right2_stem_2\n",
      "separable_conv_1_pad_reduction_right3_stem_2\n",
      "activation_208\n",
      "separable_conv_1_bn_reduction_right2_stem_2\n",
      "separable_conv_1_reduction_right3_stem_2\n",
      "separable_conv_1_reduction_left4_stem_2\n",
      "activation_205\n",
      "separable_conv_1_bn_reduction_right3_stem_2\n",
      "separable_conv_1_bn_reduction_left4_stem_2\n",
      "reduction_pad_1_stem_2\n",
      "separable_conv_2_reduction_right2_stem_2\n",
      "activation_207\n",
      "activation_209\n",
      "reduction_left2_stem_2\n",
      "separable_conv_2_bn_reduction_right2_stem_2\n",
      "separable_conv_2_reduction_right3_stem_2\n",
      "separable_conv_2_reduction_left4_stem_2\n",
      "adjust_relu_1_0\n",
      "reduction_add_2_stem_2\n",
      "reduction_left3_stem_2\n",
      "separable_conv_2_bn_reduction_right3_stem_2\n",
      "reduction_left4_stem_2\n",
      "separable_conv_2_bn_reduction_left4_stem_2\n",
      "reduction_right5_stem_2\n",
      "zero_padding2d_5\n",
      "reduction_add3_stem_2\n",
      "add_5\n",
      "reduction_add4_stem_2\n",
      "cropping2d_5\n",
      "reduction_concat_stem_2\n",
      "adjust_avg_pool_1_0\n",
      "adjust_avg_pool_2_0\n",
      "adjust_conv_1_0\n",
      "adjust_conv_2_0\n",
      "activation_210\n",
      "concatenate_5\n",
      "normal_conv_1_0\n",
      "adjust_bn_0\n",
      "normal_bn_1_0\n",
      "activation_211\n",
      "activation_213\n",
      "activation_215\n",
      "activation_217\n",
      "activation_219\n",
      "separable_conv_1_normal_left1_0\n",
      "separable_conv_1_normal_right1_0\n",
      "separable_conv_1_normal_left2_0\n",
      "separable_conv_1_normal_right2_0\n",
      "separable_conv_1_normal_left5_0\n",
      "separable_conv_1_bn_normal_left1_0\n",
      "separable_conv_1_bn_normal_right1_0\n",
      "separable_conv_1_bn_normal_left2_0\n",
      "separable_conv_1_bn_normal_right2_0\n",
      "separable_conv_1_bn_normal_left5_0\n",
      "activation_212\n",
      "activation_214\n",
      "activation_216\n",
      "activation_218\n",
      "activation_220\n",
      "separable_conv_2_normal_left1_0\n",
      "separable_conv_2_normal_right1_0\n",
      "separable_conv_2_normal_left2_0\n",
      "separable_conv_2_normal_right2_0\n",
      "separable_conv_2_normal_left5_0\n",
      "separable_conv_2_bn_normal_left1_0\n",
      "separable_conv_2_bn_normal_right1_0\n",
      "separable_conv_2_bn_normal_left2_0\n",
      "separable_conv_2_bn_normal_right2_0\n",
      "normal_left3_0\n",
      "normal_left4_0\n",
      "normal_right4_0\n",
      "separable_conv_2_bn_normal_left5_0\n",
      "normal_add_1_0\n",
      "normal_add_2_0\n",
      "normal_add_3_0\n",
      "normal_add_4_0\n",
      "normal_add_5_0\n",
      "normal_concat_0\n",
      "activation_221\n",
      "activation_222\n",
      "adjust_conv_projection_1\n",
      "normal_conv_1_1\n",
      "adjust_bn_1\n",
      "normal_bn_1_1\n",
      "activation_223\n",
      "activation_225\n",
      "activation_227\n",
      "activation_229\n",
      "activation_231\n",
      "separable_conv_1_normal_left1_1\n",
      "separable_conv_1_normal_right1_1\n",
      "separable_conv_1_normal_left2_1\n",
      "separable_conv_1_normal_right2_1\n",
      "separable_conv_1_normal_left5_1\n",
      "separable_conv_1_bn_normal_left1_1\n",
      "separable_conv_1_bn_normal_right1_1\n",
      "separable_conv_1_bn_normal_left2_1\n",
      "separable_conv_1_bn_normal_right2_1\n",
      "separable_conv_1_bn_normal_left5_1\n",
      "activation_224\n",
      "activation_226\n",
      "activation_228\n",
      "activation_230\n",
      "activation_232\n",
      "separable_conv_2_normal_left1_1\n",
      "separable_conv_2_normal_right1_1\n",
      "separable_conv_2_normal_left2_1\n",
      "separable_conv_2_normal_right2_1\n",
      "separable_conv_2_normal_left5_1\n",
      "separable_conv_2_bn_normal_left1_1\n",
      "separable_conv_2_bn_normal_right1_1\n",
      "separable_conv_2_bn_normal_left2_1\n",
      "separable_conv_2_bn_normal_right2_1\n",
      "normal_left3_1\n",
      "normal_left4_1\n",
      "normal_right4_1\n",
      "separable_conv_2_bn_normal_left5_1\n",
      "normal_add_1_1\n",
      "normal_add_2_1\n",
      "normal_add_3_1\n",
      "normal_add_4_1\n",
      "normal_add_5_1\n",
      "normal_concat_1\n",
      "activation_233\n",
      "activation_234\n",
      "adjust_conv_projection_2\n",
      "normal_conv_1_2\n",
      "adjust_bn_2\n",
      "normal_bn_1_2\n",
      "activation_235\n",
      "activation_237\n",
      "activation_239\n",
      "activation_241\n",
      "activation_243\n",
      "separable_conv_1_normal_left1_2\n",
      "separable_conv_1_normal_right1_2\n",
      "separable_conv_1_normal_left2_2\n",
      "separable_conv_1_normal_right2_2\n",
      "separable_conv_1_normal_left5_2\n",
      "separable_conv_1_bn_normal_left1_2\n",
      "separable_conv_1_bn_normal_right1_2\n",
      "separable_conv_1_bn_normal_left2_2\n",
      "separable_conv_1_bn_normal_right2_2\n",
      "separable_conv_1_bn_normal_left5_2\n",
      "activation_236\n",
      "activation_238\n",
      "activation_240\n",
      "activation_242\n",
      "activation_244\n",
      "separable_conv_2_normal_left1_2\n",
      "separable_conv_2_normal_right1_2\n",
      "separable_conv_2_normal_left2_2\n",
      "separable_conv_2_normal_right2_2\n",
      "separable_conv_2_normal_left5_2\n",
      "separable_conv_2_bn_normal_left1_2\n",
      "separable_conv_2_bn_normal_right1_2\n",
      "separable_conv_2_bn_normal_left2_2\n",
      "separable_conv_2_bn_normal_right2_2\n",
      "normal_left3_2\n",
      "normal_left4_2\n",
      "normal_right4_2\n",
      "separable_conv_2_bn_normal_left5_2\n",
      "normal_add_1_2\n",
      "normal_add_2_2\n",
      "normal_add_3_2\n",
      "normal_add_4_2\n",
      "normal_add_5_2\n",
      "normal_concat_2\n",
      "activation_245\n",
      "activation_246\n",
      "adjust_conv_projection_3\n",
      "normal_conv_1_3\n",
      "adjust_bn_3\n",
      "normal_bn_1_3\n",
      "activation_247\n",
      "activation_249\n",
      "activation_251\n",
      "activation_253\n",
      "activation_255\n",
      "separable_conv_1_normal_left1_3\n",
      "separable_conv_1_normal_right1_3\n",
      "separable_conv_1_normal_left2_3\n",
      "separable_conv_1_normal_right2_3\n",
      "separable_conv_1_normal_left5_3\n",
      "separable_conv_1_bn_normal_left1_3\n",
      "separable_conv_1_bn_normal_right1_3\n",
      "separable_conv_1_bn_normal_left2_3\n",
      "separable_conv_1_bn_normal_right2_3\n",
      "separable_conv_1_bn_normal_left5_3\n",
      "activation_248\n",
      "activation_250\n",
      "activation_252\n",
      "activation_254\n",
      "activation_256\n",
      "separable_conv_2_normal_left1_3\n",
      "separable_conv_2_normal_right1_3\n",
      "separable_conv_2_normal_left2_3\n",
      "separable_conv_2_normal_right2_3\n",
      "separable_conv_2_normal_left5_3\n",
      "separable_conv_2_bn_normal_left1_3\n",
      "separable_conv_2_bn_normal_right1_3\n",
      "separable_conv_2_bn_normal_left2_3\n",
      "separable_conv_2_bn_normal_right2_3\n",
      "normal_left3_3\n",
      "normal_left4_3\n",
      "normal_right4_3\n",
      "separable_conv_2_bn_normal_left5_3\n",
      "normal_add_1_3\n",
      "normal_add_2_3\n",
      "normal_add_3_3\n",
      "normal_add_4_3\n",
      "normal_add_5_3\n",
      "normal_concat_3\n",
      "activation_258\n",
      "activation_257\n",
      "reduction_conv_1_reduce_4\n",
      "adjust_conv_projection_reduce_4\n",
      "reduction_bn_1_reduce_4\n",
      "adjust_bn_reduce_4\n",
      "activation_259\n",
      "activation_261\n",
      "separable_conv_1_pad_reduction_left1_reduce_4\n",
      "separable_conv_1_pad_reduction_right1_reduce_4\n",
      "separable_conv_1_reduction_left1_reduce_4\n",
      "separable_conv_1_reduction_right1_reduce_4\n",
      "separable_conv_1_bn_reduction_left1_reduce_4\n",
      "separable_conv_1_bn_reduction_right1_reduce_4\n",
      "activation_260\n",
      "activation_262\n",
      "separable_conv_2_reduction_left1_reduce_4\n",
      "separable_conv_2_reduction_right1_reduce_4\n",
      "activation_263\n",
      "separable_conv_2_bn_reduction_left1_reduce_4\n",
      "separable_conv_2_bn_reduction_right1_reduce_4\n",
      "separable_conv_1_pad_reduction_right2_reduce_4\n",
      "activation_265\n",
      "reduction_add_1_reduce_4\n",
      "separable_conv_1_reduction_right2_reduce_4\n",
      "separable_conv_1_pad_reduction_right3_reduce_4\n",
      "activation_267\n",
      "separable_conv_1_bn_reduction_right2_reduce_4\n",
      "separable_conv_1_reduction_right3_reduce_4\n",
      "separable_conv_1_reduction_left4_reduce_4\n",
      "activation_264\n",
      "separable_conv_1_bn_reduction_right3_reduce_4\n",
      "separable_conv_1_bn_reduction_left4_reduce_4\n",
      "reduction_pad_1_reduce_4\n",
      "separable_conv_2_reduction_right2_reduce_4\n",
      "activation_266\n",
      "activation_268\n",
      "reduction_left2_reduce_4\n",
      "separable_conv_2_bn_reduction_right2_reduce_4\n",
      "separable_conv_2_reduction_right3_reduce_4\n",
      "separable_conv_2_reduction_left4_reduce_4\n",
      "adjust_relu_1_5\n",
      "reduction_add_2_reduce_4\n",
      "reduction_left3_reduce_4\n",
      "separable_conv_2_bn_reduction_right3_reduce_4\n",
      "reduction_left4_reduce_4\n",
      "separable_conv_2_bn_reduction_left4_reduce_4\n",
      "reduction_right5_reduce_4\n",
      "zero_padding2d_6\n",
      "reduction_add3_reduce_4\n",
      "add_6\n",
      "reduction_add4_reduce_4\n",
      "cropping2d_6\n",
      "reduction_concat_reduce_4\n",
      "adjust_avg_pool_1_5\n",
      "adjust_avg_pool_2_5\n",
      "adjust_conv_1_5\n",
      "adjust_conv_2_5\n",
      "activation_269\n",
      "concatenate_6\n",
      "normal_conv_1_5\n",
      "adjust_bn_5\n",
      "normal_bn_1_5\n",
      "activation_270\n",
      "activation_272\n",
      "activation_274\n",
      "activation_276\n",
      "activation_278\n",
      "separable_conv_1_normal_left1_5\n",
      "separable_conv_1_normal_right1_5\n",
      "separable_conv_1_normal_left2_5\n",
      "separable_conv_1_normal_right2_5\n",
      "separable_conv_1_normal_left5_5\n",
      "separable_conv_1_bn_normal_left1_5\n",
      "separable_conv_1_bn_normal_right1_5\n",
      "separable_conv_1_bn_normal_left2_5\n",
      "separable_conv_1_bn_normal_right2_5\n",
      "separable_conv_1_bn_normal_left5_5\n",
      "activation_271\n",
      "activation_273\n",
      "activation_275\n",
      "activation_277\n",
      "activation_279\n",
      "separable_conv_2_normal_left1_5\n",
      "separable_conv_2_normal_right1_5\n",
      "separable_conv_2_normal_left2_5\n",
      "separable_conv_2_normal_right2_5\n",
      "separable_conv_2_normal_left5_5\n",
      "separable_conv_2_bn_normal_left1_5\n",
      "separable_conv_2_bn_normal_right1_5\n",
      "separable_conv_2_bn_normal_left2_5\n",
      "separable_conv_2_bn_normal_right2_5\n",
      "normal_left3_5\n",
      "normal_left4_5\n",
      "normal_right4_5\n",
      "separable_conv_2_bn_normal_left5_5\n",
      "normal_add_1_5\n",
      "normal_add_2_5\n",
      "normal_add_3_5\n",
      "normal_add_4_5\n",
      "normal_add_5_5\n",
      "normal_concat_5\n",
      "activation_280\n",
      "activation_281\n",
      "adjust_conv_projection_6\n",
      "normal_conv_1_6\n",
      "adjust_bn_6\n",
      "normal_bn_1_6\n",
      "activation_282\n",
      "activation_284\n",
      "activation_286\n",
      "activation_288\n",
      "activation_290\n",
      "separable_conv_1_normal_left1_6\n",
      "separable_conv_1_normal_right1_6\n",
      "separable_conv_1_normal_left2_6\n",
      "separable_conv_1_normal_right2_6\n",
      "separable_conv_1_normal_left5_6\n",
      "separable_conv_1_bn_normal_left1_6\n",
      "separable_conv_1_bn_normal_right1_6\n",
      "separable_conv_1_bn_normal_left2_6\n",
      "separable_conv_1_bn_normal_right2_6\n",
      "separable_conv_1_bn_normal_left5_6\n",
      "activation_283\n",
      "activation_285\n",
      "activation_287\n",
      "activation_289\n",
      "activation_291\n",
      "separable_conv_2_normal_left1_6\n",
      "separable_conv_2_normal_right1_6\n",
      "separable_conv_2_normal_left2_6\n",
      "separable_conv_2_normal_right2_6\n",
      "separable_conv_2_normal_left5_6\n",
      "separable_conv_2_bn_normal_left1_6\n",
      "separable_conv_2_bn_normal_right1_6\n",
      "separable_conv_2_bn_normal_left2_6\n",
      "separable_conv_2_bn_normal_right2_6\n",
      "normal_left3_6\n",
      "normal_left4_6\n",
      "normal_right4_6\n",
      "separable_conv_2_bn_normal_left5_6\n",
      "normal_add_1_6\n",
      "normal_add_2_6\n",
      "normal_add_3_6\n",
      "normal_add_4_6\n",
      "normal_add_5_6\n",
      "normal_concat_6\n",
      "activation_292\n",
      "activation_293\n",
      "adjust_conv_projection_7\n",
      "normal_conv_1_7\n",
      "adjust_bn_7\n",
      "normal_bn_1_7\n",
      "activation_294\n",
      "activation_296\n",
      "activation_298\n",
      "activation_300\n",
      "activation_302\n",
      "separable_conv_1_normal_left1_7\n",
      "separable_conv_1_normal_right1_7\n",
      "separable_conv_1_normal_left2_7\n",
      "separable_conv_1_normal_right2_7\n",
      "separable_conv_1_normal_left5_7\n",
      "separable_conv_1_bn_normal_left1_7\n",
      "separable_conv_1_bn_normal_right1_7\n",
      "separable_conv_1_bn_normal_left2_7\n",
      "separable_conv_1_bn_normal_right2_7\n",
      "separable_conv_1_bn_normal_left5_7\n",
      "activation_295\n",
      "activation_297\n",
      "activation_299\n",
      "activation_301\n",
      "activation_303\n",
      "separable_conv_2_normal_left1_7\n",
      "separable_conv_2_normal_right1_7\n",
      "separable_conv_2_normal_left2_7\n",
      "separable_conv_2_normal_right2_7\n",
      "separable_conv_2_normal_left5_7\n",
      "separable_conv_2_bn_normal_left1_7\n",
      "separable_conv_2_bn_normal_right1_7\n",
      "separable_conv_2_bn_normal_left2_7\n",
      "separable_conv_2_bn_normal_right2_7\n",
      "normal_left3_7\n",
      "normal_left4_7\n",
      "normal_right4_7\n",
      "separable_conv_2_bn_normal_left5_7\n",
      "normal_add_1_7\n",
      "normal_add_2_7\n",
      "normal_add_3_7\n",
      "normal_add_4_7\n",
      "normal_add_5_7\n",
      "normal_concat_7\n",
      "activation_304\n",
      "activation_305\n",
      "adjust_conv_projection_8\n",
      "normal_conv_1_8\n",
      "adjust_bn_8\n",
      "normal_bn_1_8\n",
      "activation_306\n",
      "activation_308\n",
      "activation_310\n",
      "activation_312\n",
      "activation_314\n",
      "separable_conv_1_normal_left1_8\n",
      "separable_conv_1_normal_right1_8\n",
      "separable_conv_1_normal_left2_8\n",
      "separable_conv_1_normal_right2_8\n",
      "separable_conv_1_normal_left5_8\n",
      "separable_conv_1_bn_normal_left1_8\n",
      "separable_conv_1_bn_normal_right1_8\n",
      "separable_conv_1_bn_normal_left2_8\n",
      "separable_conv_1_bn_normal_right2_8\n",
      "separable_conv_1_bn_normal_left5_8\n",
      "activation_307\n",
      "activation_309\n",
      "activation_311\n",
      "activation_313\n",
      "activation_315\n",
      "separable_conv_2_normal_left1_8\n",
      "separable_conv_2_normal_right1_8\n",
      "separable_conv_2_normal_left2_8\n",
      "separable_conv_2_normal_right2_8\n",
      "separable_conv_2_normal_left5_8\n",
      "separable_conv_2_bn_normal_left1_8\n",
      "separable_conv_2_bn_normal_right1_8\n",
      "separable_conv_2_bn_normal_left2_8\n",
      "separable_conv_2_bn_normal_right2_8\n",
      "normal_left3_8\n",
      "normal_left4_8\n",
      "normal_right4_8\n",
      "separable_conv_2_bn_normal_left5_8\n",
      "normal_add_1_8\n",
      "normal_add_2_8\n",
      "normal_add_3_8\n",
      "normal_add_4_8\n",
      "normal_add_5_8\n",
      "normal_concat_8\n",
      "activation_317\n",
      "activation_316\n",
      "reduction_conv_1_reduce_8\n",
      "adjust_conv_projection_reduce_8\n",
      "reduction_bn_1_reduce_8\n",
      "adjust_bn_reduce_8\n",
      "activation_318\n",
      "activation_320\n",
      "separable_conv_1_pad_reduction_left1_reduce_8\n",
      "separable_conv_1_pad_reduction_right1_reduce_8\n",
      "separable_conv_1_reduction_left1_reduce_8\n",
      "separable_conv_1_reduction_right1_reduce_8\n",
      "separable_conv_1_bn_reduction_left1_reduce_8\n",
      "separable_conv_1_bn_reduction_right1_reduce_8\n",
      "activation_319\n",
      "activation_321\n",
      "separable_conv_2_reduction_left1_reduce_8\n",
      "separable_conv_2_reduction_right1_reduce_8\n",
      "activation_322\n",
      "separable_conv_2_bn_reduction_left1_reduce_8\n",
      "separable_conv_2_bn_reduction_right1_reduce_8\n",
      "separable_conv_1_pad_reduction_right2_reduce_8\n",
      "activation_324\n",
      "reduction_add_1_reduce_8\n",
      "separable_conv_1_reduction_right2_reduce_8\n",
      "separable_conv_1_pad_reduction_right3_reduce_8\n",
      "activation_326\n",
      "separable_conv_1_bn_reduction_right2_reduce_8\n",
      "separable_conv_1_reduction_right3_reduce_8\n",
      "separable_conv_1_reduction_left4_reduce_8\n",
      "activation_323\n",
      "separable_conv_1_bn_reduction_right3_reduce_8\n",
      "separable_conv_1_bn_reduction_left4_reduce_8\n",
      "reduction_pad_1_reduce_8\n",
      "separable_conv_2_reduction_right2_reduce_8\n",
      "activation_325\n",
      "activation_327\n",
      "reduction_left2_reduce_8\n",
      "separable_conv_2_bn_reduction_right2_reduce_8\n",
      "separable_conv_2_reduction_right3_reduce_8\n",
      "separable_conv_2_reduction_left4_reduce_8\n",
      "adjust_relu_1_9\n",
      "reduction_add_2_reduce_8\n",
      "reduction_left3_reduce_8\n",
      "separable_conv_2_bn_reduction_right3_reduce_8\n",
      "reduction_left4_reduce_8\n",
      "separable_conv_2_bn_reduction_left4_reduce_8\n",
      "reduction_right5_reduce_8\n",
      "zero_padding2d_7\n",
      "reduction_add3_reduce_8\n",
      "add_7\n",
      "reduction_add4_reduce_8\n",
      "cropping2d_7\n",
      "reduction_concat_reduce_8\n",
      "adjust_avg_pool_1_9\n",
      "adjust_avg_pool_2_9\n",
      "adjust_conv_1_9\n",
      "adjust_conv_2_9\n",
      "activation_328\n",
      "concatenate_7\n",
      "normal_conv_1_9\n",
      "adjust_bn_9\n",
      "normal_bn_1_9\n",
      "activation_329\n",
      "activation_331\n",
      "activation_333\n",
      "activation_335\n",
      "activation_337\n",
      "separable_conv_1_normal_left1_9\n",
      "separable_conv_1_normal_right1_9\n",
      "separable_conv_1_normal_left2_9\n",
      "separable_conv_1_normal_right2_9\n",
      "separable_conv_1_normal_left5_9\n",
      "separable_conv_1_bn_normal_left1_9\n",
      "separable_conv_1_bn_normal_right1_9\n",
      "separable_conv_1_bn_normal_left2_9\n",
      "separable_conv_1_bn_normal_right2_9\n",
      "separable_conv_1_bn_normal_left5_9\n",
      "activation_330\n",
      "activation_332\n",
      "activation_334\n",
      "activation_336\n",
      "activation_338\n",
      "separable_conv_2_normal_left1_9\n",
      "separable_conv_2_normal_right1_9\n",
      "separable_conv_2_normal_left2_9\n",
      "separable_conv_2_normal_right2_9\n",
      "separable_conv_2_normal_left5_9\n",
      "separable_conv_2_bn_normal_left1_9\n",
      "separable_conv_2_bn_normal_right1_9\n",
      "separable_conv_2_bn_normal_left2_9\n",
      "separable_conv_2_bn_normal_right2_9\n",
      "normal_left3_9\n",
      "normal_left4_9\n",
      "normal_right4_9\n",
      "separable_conv_2_bn_normal_left5_9\n",
      "normal_add_1_9\n",
      "normal_add_2_9\n",
      "normal_add_3_9\n",
      "normal_add_4_9\n",
      "normal_add_5_9\n",
      "normal_concat_9\n",
      "activation_339\n",
      "activation_340\n",
      "adjust_conv_projection_10\n",
      "normal_conv_1_10\n",
      "adjust_bn_10\n",
      "normal_bn_1_10\n",
      "activation_341\n",
      "activation_343\n",
      "activation_345\n",
      "activation_347\n",
      "activation_349\n",
      "separable_conv_1_normal_left1_10\n",
      "separable_conv_1_normal_right1_10\n",
      "separable_conv_1_normal_left2_10\n",
      "separable_conv_1_normal_right2_10\n",
      "separable_conv_1_normal_left5_10\n",
      "separable_conv_1_bn_normal_left1_10\n",
      "separable_conv_1_bn_normal_right1_10\n",
      "separable_conv_1_bn_normal_left2_10\n",
      "separable_conv_1_bn_normal_right2_10\n",
      "separable_conv_1_bn_normal_left5_10\n",
      "activation_342\n",
      "activation_344\n",
      "activation_346\n",
      "activation_348\n",
      "activation_350\n",
      "separable_conv_2_normal_left1_10\n",
      "separable_conv_2_normal_right1_10\n",
      "separable_conv_2_normal_left2_10\n",
      "separable_conv_2_normal_right2_10\n",
      "separable_conv_2_normal_left5_10\n",
      "separable_conv_2_bn_normal_left1_10\n",
      "separable_conv_2_bn_normal_right1_10\n",
      "separable_conv_2_bn_normal_left2_10\n",
      "separable_conv_2_bn_normal_right2_10\n",
      "normal_left3_10\n",
      "normal_left4_10\n",
      "normal_right4_10\n",
      "separable_conv_2_bn_normal_left5_10\n",
      "normal_add_1_10\n",
      "normal_add_2_10\n",
      "normal_add_3_10\n",
      "normal_add_4_10\n",
      "normal_add_5_10\n",
      "normal_concat_10\n",
      "activation_351\n",
      "activation_352\n",
      "adjust_conv_projection_11\n",
      "normal_conv_1_11\n",
      "adjust_bn_11\n",
      "normal_bn_1_11\n",
      "activation_353\n",
      "activation_355\n",
      "activation_357\n",
      "activation_359\n",
      "activation_361\n",
      "separable_conv_1_normal_left1_11\n",
      "separable_conv_1_normal_right1_11\n",
      "separable_conv_1_normal_left2_11\n",
      "separable_conv_1_normal_right2_11\n",
      "separable_conv_1_normal_left5_11\n",
      "separable_conv_1_bn_normal_left1_11\n",
      "separable_conv_1_bn_normal_right1_11\n",
      "separable_conv_1_bn_normal_left2_11\n",
      "separable_conv_1_bn_normal_right2_11\n",
      "separable_conv_1_bn_normal_left5_11\n",
      "activation_354\n",
      "activation_356\n",
      "activation_358\n",
      "activation_360\n",
      "activation_362\n",
      "separable_conv_2_normal_left1_11\n",
      "separable_conv_2_normal_right1_11\n",
      "separable_conv_2_normal_left2_11\n",
      "separable_conv_2_normal_right2_11\n",
      "separable_conv_2_normal_left5_11\n",
      "separable_conv_2_bn_normal_left1_11\n",
      "separable_conv_2_bn_normal_right1_11\n",
      "separable_conv_2_bn_normal_left2_11\n",
      "separable_conv_2_bn_normal_right2_11\n",
      "normal_left3_11\n",
      "normal_left4_11\n",
      "normal_right4_11\n",
      "separable_conv_2_bn_normal_left5_11\n",
      "normal_add_1_11\n",
      "normal_add_2_11\n",
      "normal_add_3_11\n",
      "normal_add_4_11\n",
      "normal_add_5_11\n",
      "normal_concat_11\n",
      "activation_363\n",
      "activation_364\n",
      "adjust_conv_projection_12\n",
      "normal_conv_1_12\n",
      "adjust_bn_12\n",
      "normal_bn_1_12\n",
      "activation_365\n",
      "activation_367\n",
      "activation_369\n",
      "activation_371\n",
      "activation_373\n",
      "separable_conv_1_normal_left1_12\n",
      "separable_conv_1_normal_right1_12\n",
      "separable_conv_1_normal_left2_12\n",
      "separable_conv_1_normal_right2_12\n",
      "separable_conv_1_normal_left5_12\n",
      "separable_conv_1_bn_normal_left1_12\n",
      "separable_conv_1_bn_normal_right1_12\n",
      "separable_conv_1_bn_normal_left2_12\n",
      "separable_conv_1_bn_normal_right2_12\n",
      "separable_conv_1_bn_normal_left5_12\n",
      "activation_366\n",
      "activation_368\n",
      "activation_370\n",
      "activation_372\n",
      "activation_374\n",
      "separable_conv_2_normal_left1_12\n",
      "separable_conv_2_normal_right1_12\n",
      "separable_conv_2_normal_left2_12\n",
      "separable_conv_2_normal_right2_12\n",
      "separable_conv_2_normal_left5_12\n",
      "separable_conv_2_bn_normal_left1_12\n",
      "separable_conv_2_bn_normal_right1_12\n",
      "separable_conv_2_bn_normal_left2_12\n",
      "separable_conv_2_bn_normal_right2_12\n",
      "normal_left3_12\n",
      "normal_left4_12\n",
      "normal_right4_12\n",
      "separable_conv_2_bn_normal_left5_12\n",
      "normal_add_1_12\n",
      "normal_add_2_12\n",
      "normal_add_3_12\n",
      "normal_add_4_12\n",
      "normal_add_5_12\n",
      "normal_concat_12\n",
      "activation_375\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.NASNetMobile(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    print(layer.name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:05:38.896795733Z",
     "start_time": "2024-03-24T23:05:36.271852976Z"
    }
   },
   "id": "50b9be6fe8e66d95",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from keras.src import regularizers\n",
    "\n",
    "inputs = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3), name=\"input_layer\")\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAvgPool2D()(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "outputs = layers.Dense(len(bird_classes), activation='softmax', dtype=tf.float32, kernel_regularizer=regularizers.l2(0.004))(x)\n",
    "\n",
    "model1 = tf.keras.Model(inputs, outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:05:41.661881610Z",
     "start_time": "2024-03-24T23:05:40.704614342Z"
    }
   },
   "id": "3dec416dcefcbe82",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " NASNet (Functional)         (None, 7, 7, 1056)        4269716   \n",
      "                                                                 \n",
      " global_average_pooling2d_2  (None, 1056)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 512)               541184    \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 167)               10855     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5367931 (20.48 MB)\n",
      "Trainable params: 1098215 (4.19 MB)\n",
      "Non-trainable params: 4269716 (16.29 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:05:43.435542932Z",
     "start_time": "2024-03-24T23:05:42.873100240Z"
    }
   },
   "id": "ac06abe1f3451faa",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "769"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.src.callbacks import ReduceLROnPlateau\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(factor=0.5, patience=3) \n",
    "len(base_model.layers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:05:46.483958452Z",
     "start_time": "2024-03-24T23:05:46.481349656Z"
    }
   },
   "id": "4d4381a5b1f998b7",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 19:05:54.349855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600\n",
      "2024-03-24 19:05:54.591931: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-24 19:05:55.689229: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x785d1400a4b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-24 19:05:55.689269: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2024-03-24 19:05:55.698657: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-24 19:05:55.792939: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564/564 [==============================] - ETA: 0s - loss: 4.8326 - accuracy: 0.0333\n",
      "Epoch 1: val_loss improved from inf to 3.83091, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 64s 100ms/step - loss: 4.8326 - accuracy: 0.0333 - val_loss: 3.8309 - val_accuracy: 0.1518\n",
      "Epoch 2/10\n",
      "563/564 [============================>.] - ETA: 0s - loss: 3.8255 - accuracy: 0.0871\n",
      "Epoch 2: val_loss improved from 3.83091 to 3.24719, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 57s 100ms/step - loss: 3.8253 - accuracy: 0.0872 - val_loss: 3.2472 - val_accuracy: 0.1747\n",
      "Epoch 3/10\n",
      "563/564 [============================>.] - ETA: 0s - loss: 3.4000 - accuracy: 0.1299\n",
      "Epoch 3: val_loss improved from 3.24719 to 2.98890, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 60s 107ms/step - loss: 3.4003 - accuracy: 0.1299 - val_loss: 2.9889 - val_accuracy: 0.1941\n",
      "Epoch 4/10\n",
      "564/564 [==============================] - ETA: 0s - loss: 3.1595 - accuracy: 0.1590\n",
      "Epoch 4: val_loss improved from 2.98890 to 2.74690, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 56s 98ms/step - loss: 3.1595 - accuracy: 0.1590 - val_loss: 2.7469 - val_accuracy: 0.2557\n",
      "Epoch 5/10\n",
      "563/564 [============================>.] - ETA: 0s - loss: 3.0046 - accuracy: 0.1801\n",
      "Epoch 5: val_loss improved from 2.74690 to 2.60120, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 58s 102ms/step - loss: 3.0047 - accuracy: 0.1800 - val_loss: 2.6012 - val_accuracy: 0.2372\n",
      "Epoch 6/10\n",
      "562/564 [============================>.] - ETA: 0s - loss: 2.8837 - accuracy: 0.2025\n",
      "Epoch 6: val_loss improved from 2.60120 to 2.49599, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 54s 95ms/step - loss: 2.8826 - accuracy: 0.2027 - val_loss: 2.4960 - val_accuracy: 0.2700\n",
      "Epoch 7/10\n",
      "563/564 [============================>.] - ETA: 0s - loss: 2.7731 - accuracy: 0.2262\n",
      "Epoch 7: val_loss improved from 2.49599 to 2.32300, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 57s 102ms/step - loss: 2.7731 - accuracy: 0.2262 - val_loss: 2.3230 - val_accuracy: 0.3181\n",
      "Epoch 8/10\n",
      "563/564 [============================>.] - ETA: 0s - loss: 2.6381 - accuracy: 0.2546\n",
      "Epoch 8: val_loss improved from 2.32300 to 2.29572, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 57s 100ms/step - loss: 2.6381 - accuracy: 0.2546 - val_loss: 2.2957 - val_accuracy: 0.3290\n",
      "Epoch 9/10\n",
      "563/564 [============================>.] - ETA: 0s - loss: 2.5459 - accuracy: 0.2741\n",
      "Epoch 9: val_loss improved from 2.29572 to 2.11461, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 57s 101ms/step - loss: 2.5461 - accuracy: 0.2741 - val_loss: 2.1146 - val_accuracy: 0.3739\n",
      "Epoch 10/10\n",
      "564/564 [==============================] - ETA: 0s - loss: 2.4427 - accuracy: 0.2989\n",
      "Epoch 10: val_loss improved from 2.11461 to 1.97917, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 56s 99ms/step - loss: 2.4427 - accuracy: 0.2989 - val_loss: 1.9792 - val_accuracy: 0.4255\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x785d10145810>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=Adam(),\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='Best_models/best_nasnet/', verbose=1, save_best_only=True)\n",
    "\n",
    "model1.fit(train_dataset,\n",
    "                                        validation_data=validation_dataset,\n",
    "\n",
    "                                        epochs=10,\n",
    "                                        verbose=1,\n",
    "                                        callbacks=[checkpointer]) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:15:23.436857344Z",
     "start_time": "2024-03-24T23:05:48.338999431Z"
    }
   },
   "id": "d36b50212b34becf",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for layer in base_model.layers[-100:]:\n",
    "    layer.trainable = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:15:38.494080219Z",
     "start_time": "2024-03-24T23:15:38.491516103Z"
    }
   },
   "id": "b18fc932dbae1cc2",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "564/564 [==============================] - ETA: 0s - loss: 2.3640 - accuracy: 0.3251\n",
      "Epoch 11: val_loss improved from 1.97917 to 1.90133, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 65s 99ms/step - loss: 2.3640 - accuracy: 0.3251 - val_loss: 1.9013 - val_accuracy: 0.4322 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "564/564 [==============================] - ETA: 0s - loss: 2.2809 - accuracy: 0.3415\n",
      "Epoch 12: val_loss improved from 1.90133 to 1.82014, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 58s 102ms/step - loss: 2.2809 - accuracy: 0.3415 - val_loss: 1.8201 - val_accuracy: 0.4725 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "563/564 [============================>.] - ETA: 0s - loss: 2.2071 - accuracy: 0.3610\n",
      "Epoch 13: val_loss improved from 1.82014 to 1.68646, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 60s 106ms/step - loss: 2.2074 - accuracy: 0.3611 - val_loss: 1.6865 - val_accuracy: 0.5137 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "563/564 [============================>.] - ETA: 0s - loss: 2.1424 - accuracy: 0.3767\n",
      "Epoch 14: val_loss did not improve from 1.68646\n",
      "564/564 [==============================] - 29s 52ms/step - loss: 2.1426 - accuracy: 0.3766 - val_loss: 1.7182 - val_accuracy: 0.4813 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "563/564 [============================>.] - ETA: 0s - loss: 2.0869 - accuracy: 0.3953\n",
      "Epoch 15: val_loss improved from 1.68646 to 1.61684, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 58s 103ms/step - loss: 2.0866 - accuracy: 0.3953 - val_loss: 1.6168 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "563/564 [============================>.] - ETA: 0s - loss: 2.0295 - accuracy: 0.4061\n",
      "Epoch 16: val_loss improved from 1.61684 to 1.60171, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 59s 104ms/step - loss: 2.0295 - accuracy: 0.4062 - val_loss: 1.6017 - val_accuracy: 0.5179 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "564/564 [==============================] - ETA: 0s - loss: 1.9828 - accuracy: 0.4201\n",
      "Epoch 17: val_loss improved from 1.60171 to 1.56855, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 58s 103ms/step - loss: 1.9828 - accuracy: 0.4201 - val_loss: 1.5685 - val_accuracy: 0.5140 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.9295 - accuracy: 0.4369\n",
      "Epoch 18: val_loss did not improve from 1.56855\n",
      "564/564 [==============================] - 28s 50ms/step - loss: 1.9295 - accuracy: 0.4369 - val_loss: 1.5716 - val_accuracy: 0.5476 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.8927 - accuracy: 0.4434\n",
      "Epoch 19: val_loss improved from 1.56855 to 1.44643, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 58s 103ms/step - loss: 1.8928 - accuracy: 0.4433 - val_loss: 1.4464 - val_accuracy: 0.5872 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "564/564 [==============================] - ETA: 0s - loss: 1.8449 - accuracy: 0.4617\n",
      "Epoch 20: val_loss did not improve from 1.44643\n",
      "564/564 [==============================] - 28s 50ms/step - loss: 1.8449 - accuracy: 0.4617 - val_loss: 1.4654 - val_accuracy: 0.5346 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x785d10255300>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=Adam(),\n",
    "                           metrics=['accuracy'])\n",
    "start_epoch = 10\n",
    "\n",
    "model1.fit(train_dataset,\n",
    "                                        validation_data=validation_dataset,\n",
    "                                        epochs=start_epoch+10,\n",
    "                                        initial_epoch=start_epoch, \n",
    "                                        verbose=1,\n",
    "                                        callbacks=[checkpointer, lr_scheduler]) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:24:15.810411295Z",
     "start_time": "2024-03-24T23:15:54.472776502Z"
    }
   },
   "id": "203e49fb2dead93f",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "for layer in base_model.layers[-200:]:\n",
    "    layer.trainable = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:24:22.283472646Z",
     "start_time": "2024-03-24T23:24:22.279100022Z"
    }
   },
   "id": "c9c1648ec5fea6af",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "564/564 [==============================] - ETA: 0s - loss: 1.6590 - accuracy: 0.4986\n",
      "Epoch 21: val_loss improved from 1.44643 to 1.27467, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 66s 104ms/step - loss: 1.6590 - accuracy: 0.4986 - val_loss: 1.2747 - val_accuracy: 0.6171\n",
      "Epoch 22/30\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.5819 - accuracy: 0.5170\n",
      "Epoch 22: val_loss improved from 1.27467 to 1.24272, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 57s 101ms/step - loss: 1.5818 - accuracy: 0.5170 - val_loss: 1.2427 - val_accuracy: 0.6251\n",
      "Epoch 23/30\n",
      "564/564 [==============================] - ETA: 0s - loss: 1.5364 - accuracy: 0.5270\n",
      "Epoch 23: val_loss improved from 1.24272 to 1.23374, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 57s 101ms/step - loss: 1.5364 - accuracy: 0.5270 - val_loss: 1.2337 - val_accuracy: 0.6109\n",
      "Epoch 24/30\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.4991 - accuracy: 0.5412\n",
      "Epoch 24: val_loss improved from 1.23374 to 1.20624, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 58s 102ms/step - loss: 1.4991 - accuracy: 0.5413 - val_loss: 1.2062 - val_accuracy: 0.6368\n",
      "Epoch 25/30\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.4865 - accuracy: 0.5429\n",
      "Epoch 25: val_loss improved from 1.20624 to 1.15650, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 57s 101ms/step - loss: 1.4870 - accuracy: 0.5429 - val_loss: 1.1565 - val_accuracy: 0.6416\n",
      "Epoch 26/30\n",
      "564/564 [==============================] - ETA: 0s - loss: 1.4473 - accuracy: 0.5564\n",
      "Epoch 26: val_loss did not improve from 1.15650\n",
      "564/564 [==============================] - 29s 51ms/step - loss: 1.4473 - accuracy: 0.5564 - val_loss: 1.2658 - val_accuracy: 0.6083\n",
      "Epoch 27/30\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.4257 - accuracy: 0.5614\n",
      "Epoch 27: val_loss improved from 1.15650 to 1.11567, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 60s 106ms/step - loss: 1.4259 - accuracy: 0.5613 - val_loss: 1.1157 - val_accuracy: 0.6706\n",
      "Epoch 28/30\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.3929 - accuracy: 0.5676\n",
      "Epoch 28: val_loss did not improve from 1.11567\n",
      "564/564 [==============================] - 28s 49ms/step - loss: 1.3930 - accuracy: 0.5676 - val_loss: 1.1369 - val_accuracy: 0.6872\n",
      "Epoch 29/30\n",
      "564/564 [==============================] - ETA: 0s - loss: 1.3808 - accuracy: 0.5734\n",
      "Epoch 29: val_loss did not improve from 1.11567\n",
      "564/564 [==============================] - 28s 49ms/step - loss: 1.3808 - accuracy: 0.5734 - val_loss: 1.1432 - val_accuracy: 0.6839\n",
      "Epoch 30/30\n",
      "564/564 [==============================] - ETA: 0s - loss: 1.3539 - accuracy: 0.5749\n",
      "Epoch 30: val_loss did not improve from 1.11567\n",
      "564/564 [==============================] - 28s 49ms/step - loss: 1.3539 - accuracy: 0.5749 - val_loss: 1.1546 - val_accuracy: 0.6583\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x785a24b296c0>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=Adam(0.0005),\n",
    "                           metrics=['accuracy'])\n",
    "start_epoch = 20\n",
    "model1.fit(train_dataset,\n",
    "                                        \n",
    "                                        validation_data=validation_dataset,\n",
    "                                        \n",
    "                                        epochs=start_epoch+10,\n",
    "                                        initial_epoch=start_epoch, \n",
    "                                        verbose=1,\n",
    "                                        callbacks=[checkpointer]) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:32:22.152891689Z",
     "start_time": "2024-03-24T23:24:35.503609014Z"
    }
   },
   "id": "686cc96cfff8e778",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "for layer in base_model.layers[-300:]:\n",
    "    layer.trainable = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:33:11.468028714Z",
     "start_time": "2024-03-24T23:33:11.424328419Z"
    }
   },
   "id": "f90382a212e7b9c4",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.2710 - accuracy: 0.6011\n",
      "Epoch 31: val_loss improved from 1.11567 to 1.01905, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 65s 103ms/step - loss: 1.2710 - accuracy: 0.6011 - val_loss: 1.0190 - val_accuracy: 0.6896 - lr: 3.0000e-04\n",
      "Epoch 32/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.2287 - accuracy: 0.6135\n",
      "Epoch 32: val_loss did not improve from 1.01905\n",
      "564/564 [==============================] - 28s 50ms/step - loss: 1.2287 - accuracy: 0.6135 - val_loss: 1.0211 - val_accuracy: 0.7135 - lr: 3.0000e-04\n",
      "Epoch 33/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.2120 - accuracy: 0.6176\n",
      "Epoch 33: val_loss improved from 1.01905 to 1.01899, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 58s 102ms/step - loss: 1.2121 - accuracy: 0.6176 - val_loss: 1.0190 - val_accuracy: 0.7022 - lr: 3.0000e-04\n",
      "Epoch 34/50\n",
      "562/564 [============================>.] - ETA: 0s - loss: 1.1943 - accuracy: 0.6292\n",
      "Epoch 34: val_loss did not improve from 1.01899\n",
      "564/564 [==============================] - 28s 50ms/step - loss: 1.1947 - accuracy: 0.6291 - val_loss: 1.0308 - val_accuracy: 0.6833 - lr: 3.0000e-04\n",
      "Epoch 35/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.1243 - accuracy: 0.6417\n",
      "Epoch 35: val_loss improved from 1.01899 to 0.96599, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 57s 101ms/step - loss: 1.1243 - accuracy: 0.6416 - val_loss: 0.9660 - val_accuracy: 0.7229 - lr: 1.5000e-04\n",
      "Epoch 36/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.1116 - accuracy: 0.6459\n",
      "Epoch 36: val_loss improved from 0.96599 to 0.96344, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 58s 103ms/step - loss: 1.1115 - accuracy: 0.6459 - val_loss: 0.9634 - val_accuracy: 0.7194 - lr: 1.5000e-04\n",
      "Epoch 37/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.0993 - accuracy: 0.6500\n",
      "Epoch 37: val_loss improved from 0.96344 to 0.95123, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 59s 105ms/step - loss: 1.0993 - accuracy: 0.6501 - val_loss: 0.9512 - val_accuracy: 0.7203 - lr: 1.5000e-04\n",
      "Epoch 38/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.0863 - accuracy: 0.6539\n",
      "Epoch 38: val_loss improved from 0.95123 to 0.94576, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 59s 104ms/step - loss: 1.0865 - accuracy: 0.6538 - val_loss: 0.9458 - val_accuracy: 0.7537 - lr: 1.5000e-04\n",
      "Epoch 39/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.0723 - accuracy: 0.6536\n",
      "Epoch 39: val_loss improved from 0.94576 to 0.93985, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 58s 102ms/step - loss: 1.0726 - accuracy: 0.6536 - val_loss: 0.9398 - val_accuracy: 0.7333 - lr: 1.5000e-04\n",
      "Epoch 40/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.0513 - accuracy: 0.6573\n",
      "Epoch 40: val_loss improved from 0.93985 to 0.92124, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 59s 105ms/step - loss: 1.0512 - accuracy: 0.6574 - val_loss: 0.9212 - val_accuracy: 0.7448 - lr: 1.5000e-04\n",
      "Epoch 41/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.0523 - accuracy: 0.6615\n",
      "Epoch 41: val_loss did not improve from 0.92124\n",
      "564/564 [==============================] - 29s 51ms/step - loss: 1.0523 - accuracy: 0.6615 - val_loss: 0.9224 - val_accuracy: 0.7428 - lr: 1.5000e-04\n",
      "Epoch 42/50\n",
      "564/564 [==============================] - ETA: 0s - loss: 1.0313 - accuracy: 0.6643\n",
      "Epoch 42: val_loss did not improve from 0.92124\n",
      "564/564 [==============================] - 28s 50ms/step - loss: 1.0313 - accuracy: 0.6643 - val_loss: 0.9349 - val_accuracy: 0.7520 - lr: 1.5000e-04\n",
      "Epoch 43/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.0265 - accuracy: 0.6687\n",
      "Epoch 43: val_loss improved from 0.92124 to 0.91045, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 56s 99ms/step - loss: 1.0267 - accuracy: 0.6687 - val_loss: 0.9105 - val_accuracy: 0.7474 - lr: 1.5000e-04\n",
      "Epoch 44/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.0222 - accuracy: 0.6698\n",
      "Epoch 44: val_loss improved from 0.91045 to 0.89952, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 58s 103ms/step - loss: 1.0222 - accuracy: 0.6699 - val_loss: 0.8995 - val_accuracy: 0.7576 - lr: 1.5000e-04\n",
      "Epoch 45/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.0156 - accuracy: 0.6723\n",
      "Epoch 45: val_loss did not improve from 0.89952\n",
      "564/564 [==============================] - 28s 50ms/step - loss: 1.0154 - accuracy: 0.6724 - val_loss: 0.9091 - val_accuracy: 0.7417 - lr: 1.5000e-04\n",
      "Epoch 46/50\n",
      "564/564 [==============================] - ETA: 0s - loss: 1.0067 - accuracy: 0.6797\n",
      "Epoch 46: val_loss improved from 0.89952 to 0.89773, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 58s 102ms/step - loss: 1.0067 - accuracy: 0.6797 - val_loss: 0.8977 - val_accuracy: 0.7461 - lr: 1.5000e-04\n",
      "Epoch 47/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.9877 - accuracy: 0.6787\n",
      "Epoch 47: val_loss improved from 0.89773 to 0.89551, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 59s 105ms/step - loss: 0.9877 - accuracy: 0.6786 - val_loss: 0.8955 - val_accuracy: 0.7255 - lr: 1.5000e-04\n",
      "Epoch 48/50\n",
      "564/564 [==============================] - ETA: 0s - loss: 0.9924 - accuracy: 0.6782\n",
      "Epoch 48: val_loss improved from 0.89551 to 0.89536, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 61s 108ms/step - loss: 0.9924 - accuracy: 0.6782 - val_loss: 0.8954 - val_accuracy: 0.7430 - lr: 1.5000e-04\n",
      "Epoch 49/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.9773 - accuracy: 0.6857\n",
      "Epoch 49: val_loss improved from 0.89536 to 0.89066, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 59s 104ms/step - loss: 0.9772 - accuracy: 0.6856 - val_loss: 0.8907 - val_accuracy: 0.7334 - lr: 1.5000e-04\n",
      "Epoch 50/50\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.9845 - accuracy: 0.6792\n",
      "Epoch 50: val_loss did not improve from 0.89066\n",
      "564/564 [==============================] - 28s 50ms/step - loss: 0.9844 - accuracy: 0.6792 - val_loss: 0.8939 - val_accuracy: 0.7476 - lr: 1.5000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x785a0fa7e380>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=Adam(learning_rate=0.0003),\n",
    "                           metrics=['accuracy'])\n",
    "start_epoch = 30\n",
    "\n",
    "model1.fit(train_dataset,\n",
    "                                        validation_data=validation_dataset,\n",
    "                                       \n",
    "                                        epochs=start_epoch+20,\n",
    "                                        initial_epoch=start_epoch, \n",
    "                                        verbose=1,\n",
    "                                        callbacks=[checkpointer, lr_scheduler]) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:50:01.995306176Z",
     "start_time": "2024-03-24T23:33:29.358244412Z"
    }
   },
   "id": "fc2554adc20a25a0",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "for layer in base_model.layers[-400:]:\n",
    "    layer.trainable = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:50:02.003022053Z",
     "start_time": "2024-03-24T23:50:02.001130839Z"
    }
   },
   "id": "cb754b0a48f422be",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/60\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.9589 - accuracy: 0.6906\n",
      "Epoch 51: val_loss improved from 0.89066 to 0.84903, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 66s 105ms/step - loss: 0.9588 - accuracy: 0.6907 - val_loss: 0.8490 - val_accuracy: 0.7799 - lr: 1.0000e-04\n",
      "Epoch 52/60\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.9406 - accuracy: 0.6915\n",
      "Epoch 52: val_loss did not improve from 0.84903\n",
      "564/564 [==============================] - 28s 49ms/step - loss: 0.9405 - accuracy: 0.6915 - val_loss: 0.8575 - val_accuracy: 0.7712 - lr: 1.0000e-04\n",
      "Epoch 53/60\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.9327 - accuracy: 0.6946\n",
      "Epoch 53: val_loss did not improve from 0.84903\n",
      "564/564 [==============================] - 29s 52ms/step - loss: 0.9326 - accuracy: 0.6947 - val_loss: 0.8654 - val_accuracy: 0.7404 - lr: 1.0000e-04\n",
      "Epoch 54/60\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.9371 - accuracy: 0.6935\n",
      "Epoch 54: val_loss improved from 0.84903 to 0.84321, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 56s 100ms/step - loss: 0.9370 - accuracy: 0.6936 - val_loss: 0.8432 - val_accuracy: 0.7913 - lr: 1.0000e-04\n",
      "Epoch 55/60\n",
      "564/564 [==============================] - ETA: 0s - loss: 0.9290 - accuracy: 0.6997\n",
      "Epoch 55: val_loss did not improve from 0.84321\n",
      "564/564 [==============================] - 28s 50ms/step - loss: 0.9290 - accuracy: 0.6997 - val_loss: 0.8465 - val_accuracy: 0.7646 - lr: 1.0000e-04\n",
      "Epoch 56/60\n",
      "564/564 [==============================] - ETA: 0s - loss: 0.9214 - accuracy: 0.7003\n",
      "Epoch 56: val_loss did not improve from 0.84321\n",
      "564/564 [==============================] - 29s 52ms/step - loss: 0.9214 - accuracy: 0.7003 - val_loss: 0.8618 - val_accuracy: 0.7809 - lr: 1.0000e-04\n",
      "Epoch 57/60\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.9172 - accuracy: 0.6973\n",
      "Epoch 57: val_loss did not improve from 0.84321\n",
      "564/564 [==============================] - 28s 50ms/step - loss: 0.9173 - accuracy: 0.6972 - val_loss: 0.8610 - val_accuracy: 0.7557 - lr: 1.0000e-04\n",
      "Epoch 58/60\n",
      "562/564 [============================>.] - ETA: 0s - loss: 0.8977 - accuracy: 0.7035\n",
      "Epoch 58: val_loss improved from 0.84321 to 0.83962, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 56s 100ms/step - loss: 0.8981 - accuracy: 0.7035 - val_loss: 0.8396 - val_accuracy: 0.8029 - lr: 5.0000e-05\n",
      "Epoch 59/60\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.8925 - accuracy: 0.7090\n",
      "Epoch 59: val_loss improved from 0.83962 to 0.83955, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 58s 102ms/step - loss: 0.8926 - accuracy: 0.7090 - val_loss: 0.8395 - val_accuracy: 0.7832 - lr: 5.0000e-05\n",
      "Epoch 60/60\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.8913 - accuracy: 0.7085\n",
      "Epoch 60: val_loss improved from 0.83955 to 0.83709, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 59s 104ms/step - loss: 0.8915 - accuracy: 0.7085 - val_loss: 0.8371 - val_accuracy: 0.7930 - lr: 5.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x785a03626350>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=Adam(learning_rate=0.0001),\n",
    "                           metrics=['accuracy'])\n",
    "start_epoch = 50\n",
    "\n",
    "model1.fit(train_dataset,\n",
    "                                        validation_data=validation_dataset,\n",
    "                                       \n",
    "                                        epochs=start_epoch+10,\n",
    "                                        initial_epoch=start_epoch, \n",
    "                                        verbose=1,\n",
    "                                        callbacks=[checkpointer, lr_scheduler]) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T23:57:20.347644611Z",
     "start_time": "2024-03-24T23:50:02.003418225Z"
    }
   },
   "id": "18a0e2b73538e88d",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "for layer in base_model.layers[-550:]:\n",
    "    layer.trainable = True\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd6c6f8514d7def",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/70\n",
      "564/564 [==============================] - ETA: 0s - loss: 0.8789 - accuracy: 0.7154\n",
      "Epoch 61: val_loss improved from 0.83709 to 0.83432, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 67s 107ms/step - loss: 0.8789 - accuracy: 0.7154 - val_loss: 0.8343 - val_accuracy: 0.8077 - lr: 4.0000e-05\n",
      "Epoch 62/70\n",
      "562/564 [============================>.] - ETA: 0s - loss: 0.8745 - accuracy: 0.7143\n",
      "Epoch 62: val_loss improved from 0.83432 to 0.83183, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 55s 97ms/step - loss: 0.8746 - accuracy: 0.7142 - val_loss: 0.8318 - val_accuracy: 0.7944 - lr: 4.0000e-05\n",
      "Epoch 63/70\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.8722 - accuracy: 0.7169\n",
      "Epoch 63: val_loss did not improve from 0.83183\n",
      "564/564 [==============================] - 28s 49ms/step - loss: 0.8721 - accuracy: 0.7169 - val_loss: 0.8352 - val_accuracy: 0.8099 - lr: 4.0000e-05\n",
      "Epoch 64/70\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.8774 - accuracy: 0.7158\n",
      "Epoch 64: val_loss did not improve from 0.83183\n",
      "564/564 [==============================] - 28s 50ms/step - loss: 0.8775 - accuracy: 0.7158 - val_loss: 0.8320 - val_accuracy: 0.7906 - lr: 4.0000e-05\n",
      "Epoch 65/70\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.8696 - accuracy: 0.7193\n",
      "Epoch 65: val_loss improved from 0.83183 to 0.82922, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 60s 106ms/step - loss: 0.8695 - accuracy: 0.7193 - val_loss: 0.8292 - val_accuracy: 0.7993 - lr: 4.0000e-05\n",
      "Epoch 66/70\n",
      "564/564 [==============================] - ETA: 0s - loss: 0.8631 - accuracy: 0.7193\n",
      "Epoch 66: val_loss improved from 0.82922 to 0.82692, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 56s 100ms/step - loss: 0.8631 - accuracy: 0.7193 - val_loss: 0.8269 - val_accuracy: 0.8088 - lr: 4.0000e-05\n",
      "Epoch 67/70\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.8615 - accuracy: 0.7179\n",
      "Epoch 67: val_loss did not improve from 0.82692\n",
      "564/564 [==============================] - 29s 50ms/step - loss: 0.8617 - accuracy: 0.7179 - val_loss: 0.8399 - val_accuracy: 0.7940 - lr: 4.0000e-05\n",
      "Epoch 68/70\n",
      "564/564 [==============================] - ETA: 0s - loss: 0.8694 - accuracy: 0.7162\n",
      "Epoch 68: val_loss did not improve from 0.82692\n",
      "564/564 [==============================] - 31s 55ms/step - loss: 0.8694 - accuracy: 0.7162 - val_loss: 0.8285 - val_accuracy: 0.7962 - lr: 4.0000e-05\n",
      "Epoch 69/70\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.8530 - accuracy: 0.7213\n",
      "Epoch 69: val_loss improved from 0.82692 to 0.82624, saving model to Best_models/best_nasnet/\n",
      "564/564 [==============================] - 61s 109ms/step - loss: 0.8529 - accuracy: 0.7213 - val_loss: 0.8262 - val_accuracy: 0.8086 - lr: 4.0000e-05\n",
      "Epoch 70/70\n",
      "563/564 [============================>.] - ETA: 0s - loss: 0.8517 - accuracy: 0.7217\n",
      "Epoch 70: val_loss did not improve from 0.82624\n",
      "564/564 [==============================] - 30s 53ms/step - loss: 0.8516 - accuracy: 0.7217 - val_loss: 0.8286 - val_accuracy: 0.8050 - lr: 4.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x7859fb25b550>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=Adam(learning_rate=0.00004),\n",
    "                           metrics=['accuracy'])\n",
    "start_epoch = 60\n",
    "\n",
    "model1.fit(train_dataset,\n",
    "                                        validation_data=validation_dataset,\n",
    "                                       \n",
    "                                        epochs=start_epoch+10,\n",
    "                                        initial_epoch=start_epoch, \n",
    "                                        verbose=1,\n",
    "                                        callbacks=[checkpointer, lr_scheduler]) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T00:06:09.045304610Z",
     "start_time": "2024-03-24T23:58:43.801362982Z"
    }
   },
   "id": "9fad0ba106847e6b",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "for layer in base_model.layers[-750:]:\n",
    "    layer.trainable = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T00:06:13.318810319Z",
     "start_time": "2024-03-25T00:06:13.304667215Z"
    }
   },
   "id": "100d222ebc1854c5",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/90\n",
      "563/564 [============================>.] - ETA: 0s - loss: 1.3236 - accuracy: 0.6117\n",
      "Epoch 81: val_loss did not improve from 0.80952\n",
      "564/564 [==============================] - 37s 56ms/step - loss: 1.3239 - accuracy: 0.6117 - val_loss: 0.9863 - val_accuracy: 0.7285 - lr: 1.0000e-05\n",
      "Epoch 82/90\n",
      "564/564 [==============================] - ETA: 0s - loss: 1.2248 - accuracy: 0.6371\n",
      "Epoch 82: val_loss did not improve from 0.80952\n",
      "564/564 [==============================] - 29s 51ms/step - loss: 1.2248 - accuracy: 0.6371 - val_loss: 0.9308 - val_accuracy: 0.7457 - lr: 1.0000e-05\n",
      "Epoch 83/90\n",
      "516/564 [==========================>...] - ETA: 1s - loss: 1.1683 - accuracy: 0.6536"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m model1\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      2\u001B[0m                            optimizer\u001B[38;5;241m=\u001B[39mAdam(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.00001\u001B[39m),\n\u001B[1;32m      3\u001B[0m                            metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      4\u001B[0m start_epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m80\u001B[39m\n\u001B[0;32m----> 6\u001B[0m \u001B[43mmodel1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m                                       \u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstart_epoch\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstart_epoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mcheckpointer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr_scheduler\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \n",
      "File \u001B[0;32m~/PycharmProjects/BIRD_CLASSIFICATION/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/PycharmProjects/BIRD_CLASSIFICATION/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1783\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1775\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1776\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1777\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1780\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   1781\u001B[0m ):\n\u001B[1;32m   1782\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1783\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1784\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1785\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/PycharmProjects/BIRD_CLASSIFICATION/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/PycharmProjects/BIRD_CLASSIFICATION/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    828\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    830\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 831\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    833\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    834\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/PycharmProjects/BIRD_CLASSIFICATION/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    864\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    865\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    866\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 867\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtracing_compilation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    868\u001B[0m \u001B[43m      \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_config\u001B[49m\n\u001B[1;32m    869\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    870\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    871\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    872\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    873\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/PycharmProjects/BIRD_CLASSIFICATION/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:140\u001B[0m, in \u001B[0;36mcall_function\u001B[0;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[1;32m    137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m function\u001B[38;5;241m.\u001B[39m_call_flat(  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m--> 140\u001B[0m     flat_inputs, captured_inputs\u001B[38;5;241m=\u001B[39m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\n\u001B[1;32m    141\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/BIRD_CLASSIFICATION/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1449\u001B[0m, in \u001B[0;36mConcreteFunction.captured_inputs\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1443\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m   1444\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcaptured_inputs\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1445\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Returns external Tensors captured by this function.\u001B[39;00m\n\u001B[1;32m   1446\u001B[0m \n\u001B[1;32m   1447\u001B[0m \u001B[38;5;124;03m  self.__call__(*args) passes `args + self.captured_inputs` to the function.\u001B[39;00m\n\u001B[1;32m   1448\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1449\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflatten\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1450\u001B[0m \u001B[43m      \u001B[49m\u001B[43m[\u001B[49m\u001B[43mx\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mcallable\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_captured_inputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1451\u001B[0m \u001B[43m      \u001B[49m\u001B[43mexpand_composites\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/BIRD_CLASSIFICATION/.venv/lib/python3.10/site-packages/tensorflow/python/util/nest.py:294\u001B[0m, in \u001B[0;36mflatten\u001B[0;34m(structure, expand_composites)\u001B[0m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnest.flatten\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mflatten\u001B[39m(structure, expand_composites\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    201\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Returns a flat list from a given structure.\u001B[39;00m\n\u001B[1;32m    202\u001B[0m \n\u001B[1;32m    203\u001B[0m \u001B[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;124;03m    TypeError: The nest is or contains a dict with non-sortable keys.\u001B[39;00m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 294\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnest_util\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflatten\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    295\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnest_util\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mModality\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCORE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstructure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpand_composites\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/BIRD_CLASSIFICATION/.venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:815\u001B[0m, in \u001B[0;36mflatten\u001B[0;34m(modality, structure, expand_composites)\u001B[0m\n\u001B[1;32m    719\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Flattens a nested structure.\u001B[39;00m\n\u001B[1;32m    720\u001B[0m \n\u001B[1;32m    721\u001B[0m \u001B[38;5;124;03m- For Modality.CORE: refer to\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;124;03m  TypeError: The nest is or contains a dict with non-sortable keys.\u001B[39;00m\n\u001B[1;32m    813\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    814\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m modality \u001B[38;5;241m==\u001B[39m Modality\u001B[38;5;241m.\u001B[39mCORE:\n\u001B[0;32m--> 815\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_tf_core_flatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstructure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpand_composites\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    816\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m modality \u001B[38;5;241m==\u001B[39m Modality\u001B[38;5;241m.\u001B[39mDATA:\n\u001B[1;32m    817\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _tf_data_flatten(structure)\n",
      "File \u001B[0;32m~/PycharmProjects/BIRD_CLASSIFICATION/.venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:829\u001B[0m, in \u001B[0;36m_tf_core_flatten\u001B[0;34m(structure, expand_composites)\u001B[0m\n\u001B[1;32m    827\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;01mNone\u001B[39;00m]\n\u001B[1;32m    828\u001B[0m expand_composites \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mbool\u001B[39m(expand_composites)\n\u001B[0;32m--> 829\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_pywrap_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFlatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstructure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpand_composites\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model1.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=Adam(learning_rate=0.00001),\n",
    "                           metrics=['accuracy'])\n",
    "start_epoch = 80\n",
    "\n",
    "model1.fit(train_dataset,\n",
    "                                        validation_data=validation_dataset,\n",
    "                                       \n",
    "                                        epochs=start_epoch+10,\n",
    "                                        initial_epoch=start_epoch, \n",
    "                                        verbose=1,\n",
    "                                        callbacks=[checkpointer, lr_scheduler]) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T00:15:40.749910083Z",
     "start_time": "2024-03-25T00:14:15.084717981Z"
    }
   },
   "id": "b401f639a07dcf71",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 14:46:44.536428: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-24 14:46:44.564019: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-24 14:46:44.564090: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-24 14:46:44.564134: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-24 14:46:44.570442: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-24 14:46:45.410596: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11545 files belonging to 167 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 14:46:46.559885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 14:46:46.588070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 14:46:46.588288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 14:46:46.589183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 14:46:46.589344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 14:46:46.589488: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 14:46:46.635350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 14:46:46.635625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 14:46:46.635843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-24 14:46:46.635987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4617 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11545 files belonging to 167 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "test_dataset_224 = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_data_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(224, 224),\n",
    "    shuffle=False,\n",
    "    seed=123,\n",
    "\n",
    ")\n",
    "test_dataset_299 = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_data_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(299, 299),\n",
    "    shuffle=False,\n",
    "    seed=123,\n",
    "\n",
    ")\n",
    "preprocess_input = tf.keras.applications.mobilenet_v3.preprocess_input\n",
    "\n",
    "test_dataset_224 = test_dataset_224.map(lambda x, y: (preprocess_input(x), y))\n",
    "test_dataset_299 = test_dataset_299.map(lambda x, y: (preprocess_input(x), y))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f5f2fa1cb552d28",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_labels_224 = []\n",
    "y_labels_299 = []\n",
    "for images, labels in test_dataset_224.unbatch():\n",
    "    y_labels_224.append(labels.numpy().argmax())\n",
    "for images, labels in test_dataset_299.unbatch():\n",
    "    y_labels_299.append(labels.numpy().argmax())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fef6cae17abf5a68",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculate_accuracy_top3(row):\n",
    "    if row['True Label'] in row['top 3 cols']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67f1aa1c789bca7d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361/361 [==============================] - 52s 136ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.9887397141619749"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eff_b7 = tf.keras.models.load_model(\"Best_models/best_eff_b7_90.67\")\n",
    "preds_eff_b7 = pd.DataFrame(eff_b7.predict(test_dataset_224))\n",
    "top3_labels = utils.add_top_three_row_values(preds_eff_b7, y_labels_224)\n",
    "top3_labels['Accuracy_Top3'] = top3_labels.apply(calculate_accuracy_top3, axis=1)\n",
    "overall_accuracy_top3 = top3_labels['Accuracy_Top3'].mean()\n",
    "overall_accuracy_top3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T18:58:27.288892407Z",
     "start_time": "2024-03-24T18:57:17.137700522Z"
    }
   },
   "id": "aadfb66295325487",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361/361 [==============================] - 42s 107ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.9799913382416631"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eff_v2L = tf.keras.models.load_model(\"Best_models/best_effnetv2L_90.57\")\n",
    "\n",
    "preds_eff_v2L = pd.DataFrame(eff_v2L.predict(test_dataset_224))\n",
    "\n",
    "top3_labels_effv2L = utils.add_top_three_row_values(preds_eff_v2L, y_labels_224)\n",
    "\n",
    "top3_labels_effv2L['Accuracy_Top3'] = top3_labels_effv2L.apply(calculate_accuracy_top3, axis=1)\n",
    "\n",
    "overall_accuracy_top3_effv2L = top3_labels_effv2L['Accuracy_Top3'].mean()\n",
    "\n",
    "overall_accuracy_top3_effv2L"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T19:02:35.464218117Z",
     "start_time": "2024-03-24T19:01:25.365695232Z"
    }
   },
   "id": "f0bed9c2b7047aea",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361/361 [==============================] - 5s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.9711563447379818"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenet_vsmall = tf.keras.models.load_model(\"Best_models/best_mobilenet_small_89.65\")\n",
    "\n",
    "preds_mobilenet_vsmall= pd.DataFrame(mobilenet_vsmall.predict(test_dataset_224))\n",
    "\n",
    "top3_mobilenet_vsmall  = utils.add_top_three_row_values(preds_mobilenet_vsmall, y_labels_224)\n",
    "\n",
    "top3_mobilenet_vsmall['Accuracy_Top3'] = top3_mobilenet_vsmall.apply(calculate_accuracy_top3, axis=1)\n",
    "\n",
    "overall_accuracy_top3_mobilenet_vsmall= top3_mobilenet_vsmall['Accuracy_Top3'].mean()\n",
    "\n",
    "overall_accuracy_top3_mobilenet_vsmall"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T19:03:30.134320558Z",
     "start_time": "2024-03-24T19:03:21.341518113Z"
    }
   },
   "id": "574408f4edaf7a60",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361/361 [==============================] - 7s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.9778258986574274"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenet_vlarge = tf.keras.models.load_model(\"Best_models/best_mobilenetv3large_90.67\")\n",
    "\n",
    "preds_mobilenet_vlarge = pd.DataFrame(mobilenet_vlarge.predict(test_dataset_224))\n",
    "\n",
    "top3_mobilenet_vlarge  = utils.add_top_three_row_values(preds_mobilenet_vlarge, y_labels_224)\n",
    "\n",
    "top3_mobilenet_vlarge['Accuracy_Top3'] = top3_mobilenet_vlarge.apply(calculate_accuracy_top3, axis=1)\n",
    "\n",
    "overall_accuracy_top3_mobilenet_vlarge= top3_mobilenet_vlarge['Accuracy_Top3'].mean()\n",
    "\n",
    "overall_accuracy_top3_mobilenet_vlarge"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T19:05:29.575781066Z",
     "start_time": "2024-03-24T19:05:18.779974966Z"
    }
   },
   "id": "771f86763b648bc8",
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
